{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [Private Score 0.59168]Simple FE + Autogluon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "import sys\n",
    "import os\n",
    "% % capture\n",
    "!pip install autogluon.tabular\n",
    "!pip install pytest-warnings\n",
    "\n",
    "\n",
    "warnings.filterwarnings('ignore')\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# read in data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('./tabular-playground-series-aug-2022/train.csv')\n",
    "test_features = pd.read_csv('./tabular-playground-series-aug-2022/test.csv')\n",
    "train_features = train.iloc[0:, 0:-1]\n",
    "all_features = pd.concat([train_features, test_features])\n",
    "all_features['m_3_missing'] = all_features.measurement_3.isna()\n",
    "all_features['m_5_missing'] = all_features.measurement_5.isna()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "null_features = [\n",
    "    col for col in all_features.columns if all_features[col].isnull().sum() != 0]\n",
    "nonull_features = [\n",
    "    col for col in all_features.columns if col not in null_features]\n",
    "object_features = [\n",
    "    col for col in train.columns if train[col].dtypes == 'object']\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LGBMImputer  - fill in missing values\n",
    "- 關於資料的分析詳細寫在 report 中\n",
    "- LGBMImputer : Regression imputer for missing values using LightGBM.\n",
    "- 參考 github 網址 : https://github.com/analokmaus/kuma_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fatal: destination path 'kuma_utils' already exists and is not an empty directory.\n"
     ]
    }
   ],
   "source": [
    "from kuma_utils.preprocessing.imputer import LGBMImputer\n",
    "!git clone https: // github.com/analokmaus/kuma_utils.git\n",
    "sys.path.append(\"kuma_utils/\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I'], dtype=object)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(all_features['product_code'].unique())\n",
    "df_A = all_features[all_features['product_code'] == 'A']\n",
    "df_B = all_features[all_features['product_code'] == 'B']\n",
    "df_C = all_features[all_features['product_code'] == 'C']\n",
    "df_D = all_features[all_features['product_code'] == 'D']\n",
    "df_E = all_features[all_features['product_code'] == 'E']\n",
    "df_F = all_features[all_features['product_code'] == 'F']\n",
    "df_G = all_features[all_features['product_code'] == 'G']\n",
    "df_H = all_features[all_features['product_code'] == 'H']\n",
    "df_I = all_features[all_features['product_code'] == 'I']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f1341d4f26ab4ef8afa0e3fdd7f01876",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/16 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "15b6b2e4c0c446bb8283cba80bf844ac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/16 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4c6646bd20e142aeaa0d0636c06a779d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/16 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9338bcd4dd1c4876a478a7b4e374a03c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/16 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "05c2ca24c69b414d8c525897803012bb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/16 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "919e215f3a6b41f1a62ce02c875492da",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/16 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d241cbb364d5451cbdf6392deb2ead2c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/16 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b2bacdb506484e7a9fb7c3a8c9618e23",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/16 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7dedb63640d846cb81788452f94e35e5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/16 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "lgbm_imtr = LGBMImputer(cat_features=object_features, n_iter=50)\n",
    "\n",
    "# train dataset\n",
    "train_iterimp_A = lgbm_imtr.fit_transform(df_A[null_features])\n",
    "train_iterimp_B = lgbm_imtr.fit_transform(df_B[null_features])\n",
    "train_iterimp_C = lgbm_imtr.fit_transform(df_C[null_features])\n",
    "train_iterimp_D = lgbm_imtr.fit_transform(df_D[null_features])\n",
    "train_iterimp_E = lgbm_imtr.fit_transform(df_E[null_features])\n",
    "\n",
    "# test dataset\n",
    "test_iterimp_F = lgbm_imtr.fit_transform(df_F[null_features])\n",
    "test_iterimp_G = lgbm_imtr.fit_transform(df_G[null_features])\n",
    "test_iterimp_H = lgbm_imtr.fit_transform(df_H[null_features])\n",
    "test_iterimp_I = lgbm_imtr.fit_transform(df_I[null_features])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_features_ = pd.concat([train_iterimp_A, train_iterimp_B, train_iterimp_C, train_iterimp_D,\n",
    "                           train_iterimp_E, test_iterimp_F, test_iterimp_G, test_iterimp_H, test_iterimp_I\n",
    "                           ])\n",
    "all_features = pd.concat(\n",
    "    [all_features_, all_features[nonull_features]], axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing values in train dataset after pre-peocessing is:  0\n"
     ]
    }
   ],
   "source": [
    "print(\"Missing values in train dataset after pre-peocessing is: \",\n",
    "      format(all_features.isna().sum().sum()))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# choose training data columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_features['a_2*3'] = all_features['attribute_2'] * \\\n",
    "    all_features['attribute_3']\n",
    "\n",
    "meas_gr1_cols = [f\"measurement_{i:d}\" for i in list(\n",
    "    range(3, 5)) + list(range(9, 17))]\n",
    "all_features['meas_gr1_avg'] = np.mean(all_features[meas_gr1_cols], axis=1)\n",
    "all_features['meas_gr1_std'] = np.std(all_features[meas_gr1_cols], axis=1)\n",
    "\n",
    "meas_gr2_cols = [f\"measurement_{i:d}\" for i in list(range(5, 9))]\n",
    "all_features['meas_gr2_avg'] = np.mean(all_features[meas_gr2_cols], axis=1)\n",
    "all_features['meas_gr2_avg'] = np.mean(all_features[meas_gr2_cols], axis=1)\n",
    "\n",
    "all_features['meas17/meas_gr2_avg'] = all_features['measurement_17'] / \\\n",
    "    all_features['meas_gr2_avg']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_to_use = ['attribute_0', 'measurement_0', 'measurement_1', 'measurement_2', 'attribute_1', 'm_3_missing', 'm_5_missing', 'meas_gr1_avg', 'meas_gr1_std',\n",
    "               'a_2*3', 'loading', 'measurement_17']  # , 'meas17/meas_gr2_avg'\n",
    "all_features = all_features[cols_to_use]\n",
    "all_features = pd.get_dummies(\n",
    "    all_features, columns=['attribute_0', 'attribute_1'])\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# training using autogluon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from autogluon.tabular import TabularPredictor, TabularDataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = train['failure']\n",
    "train = all_features.iloc[:labels.shape[0]]\n",
    "test = all_features.iloc[labels.shape[0]:]\n",
    "\n",
    "train['label'] = labels\n",
    "train = TabularDataset(train)\n",
    "test = TabularDataset(test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No path specified. Models will be saved in: \"AutogluonModels/ag-20230107_133316/\"\n",
      "Presets specified: ['best_quality', 'optimize_for_deployment']\n",
      "Stack configuration (auto_stack=True): num_stack_levels=0, num_bag_folds=8, num_bag_sets=1\n",
      "Beginning AutoGluon training ...\n",
      "AutoGluon will save models to \"AutogluonModels/ag-20230107_133316/\"\n",
      "AutoGluon Version:  0.6.1\n",
      "Python Version:     3.8.10\n",
      "Operating System:   Linux\n",
      "Platform Machine:   x86_64\n",
      "Platform Version:   #2 SMP Thu Oct 6 15:52:22 CST 2022\n",
      "Train Data Rows:    26570\n",
      "Train Data Columns: 16\n",
      "Label Column: label\n",
      "Preprocessing data ...\n",
      "AutoGluon infers your prediction problem is: 'binary' (because only two unique label-values observed).\n",
      "\t2 unique label values:  [0, 1]\n",
      "\tIf 'binary' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n",
      "Selected class <--> label mapping:  class 1 = 1, class 0 = 0\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    3630.19 MB\n",
      "\tTrain Data (Original)  Memory Usage: 1.91 MB (0.1% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\t\t\tNote: Converting 7 features to boolean dtype as they only contain 2 unique values.\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tUseless Original Features (Count: 1): ['attribute_1_material_7']\n",
      "\t\tThese features carry no predictive signal and should be manually investigated.\n",
      "\t\tThis is typically a feature which has the same value for all rows.\n",
      "\t\tThese features do not need to be present at inference time.\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('bool', [])  : 2 | ['m_3_missing', 'm_5_missing']\n",
      "\t\t('float', []) : 4 | ['meas_gr1_avg', 'meas_gr1_std', 'loading', 'measurement_17']\n",
      "\t\t('int', [])   : 9 | ['measurement_0', 'measurement_1', 'measurement_2', 'a_2*3', 'attribute_0_material_5', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', [])     : 4 | ['meas_gr1_avg', 'meas_gr1_std', 'loading', 'measurement_17']\n",
      "\t\t('int', [])       : 4 | ['measurement_0', 'measurement_1', 'measurement_2', 'a_2*3']\n",
      "\t\t('int', ['bool']) : 7 | ['m_3_missing', 'm_5_missing', 'attribute_0_material_5', 'attribute_0_material_7', 'attribute_1_material_5', ...]\n",
      "\t0.1s = Fit runtime\n",
      "\t15 features in original data used to generate 15 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 1.89 MB (0.1% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.09s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'roc_auc'\n",
      "\tThis metric expects predicted probabilities rather than predicted class labels, so you'll need to use predict_proba() instead of predict()\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Fitting 13 L1 models ...\n",
      "Fitting model: KNeighborsUnif_BAG_L1 ...\n",
      "\t0.5225\t = Validation score   (roc_auc)\n",
      "\t0.02s\t = Training   runtime\n",
      "\t0.16s\t = Validation runtime\n",
      "Fitting model: KNeighborsDist_BAG_L1 ...\n",
      "\t0.521\t = Validation score   (roc_auc)\n",
      "\t0.02s\t = Training   runtime\n",
      "\t0.15s\t = Validation runtime\n",
      "Fitting model: LightGBMXT_BAG_L1 ...\n",
      "Will use sequential fold fitting strategy because import of ray failed. Reason: ray is required to train folds in parallel. A quick tip is to install via `pip install ray==2.0.0`, or use sequential fold fitting by passing `sequential_local` to `ag_args_ensemble` when calling tabular.fitFor example: `predictor.fit(..., ag_args_ensemble={'fold_fitting_strategy': 'sequential_local'})`\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy\n",
      "\t0.5857\t = Validation score   (roc_auc)\n",
      "\t1.95s\t = Training   runtime\n",
      "\t0.06s\t = Validation runtime\n",
      "Fitting model: LightGBM_BAG_L1 ...\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy\n",
      "\t0.5816\t = Validation score   (roc_auc)\n",
      "\t1.74s\t = Training   runtime\n",
      "\t0.04s\t = Validation runtime\n",
      "Fitting model: RandomForestGini_BAG_L1 ...\n",
      "\t0.5429\t = Validation score   (roc_auc)\n",
      "\t1.85s\t = Training   runtime\n",
      "\t0.69s\t = Validation runtime\n",
      "Fitting model: RandomForestEntr_BAG_L1 ...\n",
      "\t0.5462\t = Validation score   (roc_auc)\n",
      "\t2.36s\t = Training   runtime\n",
      "\t0.65s\t = Validation runtime\n",
      "Fitting model: CatBoost_BAG_L1 ...\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy\n",
      "\t0.5872\t = Validation score   (roc_auc)\n",
      "\t5.81s\t = Training   runtime\n",
      "\t0.02s\t = Validation runtime\n",
      "Fitting model: ExtraTreesGini_BAG_L1 ...\n",
      "\t0.5415\t = Validation score   (roc_auc)\n",
      "\t1.25s\t = Training   runtime\n",
      "\t0.76s\t = Validation runtime\n",
      "Fitting model: ExtraTreesEntr_BAG_L1 ...\n",
      "\t0.5431\t = Validation score   (roc_auc)\n",
      "\t1.21s\t = Training   runtime\n",
      "\t0.74s\t = Validation runtime\n",
      "Fitting model: NeuralNetFastAI_BAG_L1 ...\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy\n",
      "\tWarning: Exception caused NeuralNetFastAI_BAG_L1 to fail during training (ImportError)... Skipping this model.\n",
      "\t\tImport fastai failed. A quick tip is to install via `pip install autogluon.tabular[fastai]==0.6.1`. \n",
      "Fitting model: XGBoost_BAG_L1 ...\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy\n",
      "\t0.5742\t = Validation score   (roc_auc)\n",
      "\t2.87s\t = Training   runtime\n",
      "\t0.11s\t = Validation runtime\n",
      "Fitting model: NeuralNetTorch_BAG_L1 ...\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy\n",
      "\t0.5882\t = Validation score   (roc_auc)\n",
      "\t64.05s\t = Training   runtime\n",
      "\t0.08s\t = Validation runtime\n",
      "Fitting model: LightGBMLarge_BAG_L1 ...\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy\n",
      "\t0.5753\t = Validation score   (roc_auc)\n",
      "\t3.93s\t = Training   runtime\n",
      "\t0.03s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ...\n",
      "\t0.5898\t = Validation score   (roc_auc)\n",
      "\t3.11s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 100.14s ... Best model: \"WeightedEnsemble_L2\"\n",
      "Deleting model KNeighborsUnif_BAG_L1. All files under AutogluonModels/ag-20230107_133316/models/KNeighborsUnif_BAG_L1/ will be removed.\n",
      "Deleting model KNeighborsDist_BAG_L1. All files under AutogluonModels/ag-20230107_133316/models/KNeighborsDist_BAG_L1/ will be removed.\n",
      "Deleting model RandomForestGini_BAG_L1. All files under AutogluonModels/ag-20230107_133316/models/RandomForestGini_BAG_L1/ will be removed.\n",
      "Deleting model RandomForestEntr_BAG_L1. All files under AutogluonModels/ag-20230107_133316/models/RandomForestEntr_BAG_L1/ will be removed.\n",
      "Deleting model ExtraTreesGini_BAG_L1. All files under AutogluonModels/ag-20230107_133316/models/ExtraTreesGini_BAG_L1/ will be removed.\n",
      "Deleting model ExtraTreesEntr_BAG_L1. All files under AutogluonModels/ag-20230107_133316/models/ExtraTreesEntr_BAG_L1/ will be removed.\n",
      "Deleting model XGBoost_BAG_L1. All files under AutogluonModels/ag-20230107_133316/models/XGBoost_BAG_L1/ will be removed.\n",
      "Deleting model LightGBMLarge_BAG_L1. All files under AutogluonModels/ag-20230107_133316/models/LightGBMLarge_BAG_L1/ will be removed.\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels/ag-20230107_133316/\")\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels/ag-20230107_133316/\")\n"
     ]
    }
   ],
   "source": [
    "predictor = TabularPredictor(label=\"label\", eval_metric=\"roc_auc\")\n",
    "predictor.fit(train,  presets=[\"best_quality\", \"optimize_for_deployment\"])\n",
    "\n",
    "# 查看各個模型的測試效果\n",
    "# predictor.leaderboard()\n",
    "\n",
    "# 儲存模型\n",
    "predictor.save()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10 (default, Jun 22 2022, 20:18:18) \n[GCC 9.4.0]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
